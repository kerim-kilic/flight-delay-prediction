---
title: "Study of delay prediction in the US airport network"
author: "Kerim Kili√ß"
subtitle: "Supervised Machined Learning using flight data"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 2
    number_sections: true
    toc_float: true
---

# Libraries

The following three libraries are used in this R markdown file.

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidymodels)
library(sparklyr)
```

# Initialize spark and read in data

```{r}
### Uncomment these two lines to install spark, 
### and optionally to disconnect an existing connection
# spark_install(version = "3.3.0")
# spark_disconnect(sc)
sc <- spark_connect(master = "local")

### Uncomment these two lines to read in the RDS file of the data of 2017
# my_data <- readRDS("flights_2017.RDS")
# fwrite(my_data,file = "flights_2017.csv")
raw_data <- spark_read_csv(sc,"flights_2017.csv")
raw_data %>% glimpse()
```

# Clean and classify data

```{r}
### Remove redundant variables and classify the data based on the numerical delays
spark_data <- raw_data %>%
  select(-id,-year,-wheels_on_time,-wheels_off_time,-planned_departure_time,-planned_arrival_time,
         -actual_departure_time,-actual_arrival_time,-tail_number,-flight_number,-carrier) %>%
  mutate(delay = case_when(carrier_delay + weather_delay + nas_delay + security_delay + late_aircraft_delay > 0 ~ "1",
                           carrier_delay + weather_delay + nas_delay + security_delay + late_aircraft_delay == 0 ~ "0"))

### Remove the numerical delays since we only look at classification for now
spark_data <- spark_data %>%
  select(-carrier_delay,-weather_delay,-nas_delay,-security_delay,-late_aircraft_delay)
### Remove rows that contain any number of NA values
spark_data <- na.omit(spark_data)
spark_data %>% glimpse()

### Index the categorical features and apply one hot encoding
spark_data <- ft_string_indexer(spark_data, input_col = "origin", output_col = "origin_ind")
spark_data <- ft_string_indexer(spark_data, input_col = "destination", output_col = "destination_ind")
spark_data_one_hot_encoded <- ft_one_hot_encoder(spark_data, input_cols = c("origin_ind","destination_ind"), output_cols = c("origin_out","destination_out")) %>%
  select(-origin,-destination)
spark_data_one_hot_encoded %>% glimpse()
```

# Splitting data in train and test sets

Take a sample set of the data to reduce time to train the model since we are just playing around with the package.

```{r}
train_data <- spark_data_one_hot_encoded %>%
  group_by(delay) %>%
  sample_frac(0.0008)

test_data <- spark_data_one_hot_encoded %>%
  group_by(delay) %>%
  sample_frac(0.0002)

train_data %>% glimpse()
```

# Logistic regression model

In this section we will build a logistic regression pipeline and cross-validate and hyper-parameter tune a logistic regression model.

## Building a logistic regression pipeline 

Below we build a ML pipeline for a logistic regression model to use cross validation as we perform hyper parameter tuning on our model.

```{r}
# Pipeline
glm_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_logistic_regression()

# Grid
grid <- list(logistic_regression = list(elastic_net_param = c(0,0.25,0.5,0.75,1), reg_param = c(0,0.25,0.5,0.75,1)))

# Cross validation
cv <- ml_cross_validator(
  sc,
  estimator = glm_pipeline, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_multiclass_classification_evaluator(sc,metric_name = "accuracy"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
)

cv_model <- ml_fit(cv, train_data)

a <- cv_model$avg_metrics_df
```


## Train a tuned logistic regression model

Train a logistic regression model using the full training data set and the parameters that rolled out of the cross validation with hyper model parameter tuning.

```{r}
### Train a logistic regression model ###
glm_model <- ml_logistic_regression(train_data, "delay ~ .",
                                    elastic_net_param = a[which.max(a$accuracy),"elastic_net_param_1"],
                                    reg_param = a[which.max(a$accuracy),"reg_param_1"])
### Performance on train set
augment_value <- augment(glm_model) %>% collect()
augment_value$delay <- as.factor(augment_value$delay)
augment_value$.predicted_label <- as.factor(augment_value$.predicted_label)
augment_value %>%
  accuracy(estimate = .predicted_label, truth = delay)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
ml_predict(glm_model,test_data) %>%
  ml_metrics_multiclass()
```

# Decision tree model

In this section we will build a decision tree pipeline and cross-validate and hyper-parameter tune a decision tree model.

## Building a decision tree pipeline

```{r}
# Pipeline
tree_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_decision_tree_classifier()

# Grid
grid <- list(decision_tree = list(max_depth = c(1,3,5,7,10), max_bins = 317))

# Cross validation
cv <- ml_cross_validator(
  sc,
  estimator = tree_pipeline, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_multiclass_classification_evaluator(sc,metric_name = "accuracy"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
)

cv_model <- ml_fit(cv, train_data)

a <- cv_model$avg_metrics_df
```


## Train a decision tree model

Train a decision tree model using the train dataset and get model performance on training data.

```{r}
### Train a decision tree model ###
tree_model <- ml_decision_tree_classifier(train_data, 
                                          "delay ~ .", 
                                          max_bins = a[which.max(a$accuracy),"max_bins_1"],
                                          max_depth = a[which.max(a$accuracy),"max_depth_1"])
### Performance on train set
augment_value <- augment(tree_model) %>% collect()
augment_value$delay <- as.factor(augment_value$delay)
augment_value$.predicted_label <- as.factor(augment_value$.predicted_label)
augment_value %>%
  accuracy(estimate = .predicted_label, truth = delay)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
ml_predict(tree_model,test_data) %>%
  ml_metrics_multiclass()
```

# Random forest model

In this section we will build a random forest pipeline and cross-validate and hyper-parameter tune a random forest model.

## Building a decision tree pipeline

```{r}
# Pipeline
rf_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_random_forest_classifier()

# Grid
grid <- list(random_forest = list(max_depth = c(1,3,5,7,10), num_trees = c(1,3,5,7,10,25,50), max_bins = 317))

# Cross validation
cv <- ml_cross_validator(
  sc,
  estimator = rf_pipeline, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_multiclass_classification_evaluator(sc,metric_name = "accuracy"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
)

cv_model <- ml_fit(cv, train_data)

a <- cv_model$avg_metrics_df
```

## Train a random forest model

Train a random forest model using the train dataset and get model performance on training data.

```{r}
### Train a decision tree model ###
rf_model <- ml_random_forest_classifier(train_data, 
                                        "delay ~ .", 
                                        num_trees = a[which.max(a$accuracy),"num_trees_1"],
                                        max_depth = a[which.max(a$accuracy),"max_depth_1"],
                                        max_bins = 317)
### Performance on train set
augment_value <- augment(rf_model) %>% collect()
augment_value$delay <- as.factor(augment_value$delay)
augment_value$.predicted_label <- as.factor(augment_value$.predicted_label)
augment_value %>%
  accuracy(estimate = .predicted_label, truth = delay)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
ml_predict(rf_model,test_data) %>%
  ml_metrics_multiclass()
```