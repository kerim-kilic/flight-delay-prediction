---
title: "Study of delay prediction in the US airport network"
author: "Kerim Kili√ß"
subtitle: "Supervised Machined Learning using flight data"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 2
    number_sections: true
    toc_float: true
---

# Libraries

The following libraries are used in this R markdown file.

```{r setup, message=FALSE}
markdown_start_time <- Sys.time()
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidymodels)
library(sparklyr)
library(kableExtra)
source("src/functions.R")
library(h2o)
library(rsparkling)
```

# Initialize spark and h2o 

## Initialize spark and read in flights data

```{r}
### Check if spark installation exists, if not install correct version
if(!spark_install_find("3.3.0")$installed)
{
  spark_install(version = "3.3.0")
}

spark_config <- spark_config()
spark_config$'sparklyr.shell.driver-memory' <- "12G"

sc <- spark_connect(master = "local",
                    config = spark_config,
                    version = "3.3.0")

### Check if csv of the data file exists, if not create one.
if(!file.exists("data/flights_2017.csv"))
{
  my_data <- readRDS("data/flights_2017.RDS")
  fwrite(my_data,file = "data/flights_2017.csv")  
}

raw_data <- spark_read_csv(sc,"data/flights_2017.csv")
raw_data %>% glimpse()
```

## Initialize h2o 

```{r, message=FALSE, fig.show='hide', results='hide'}
h2oConf <- rsparkling::H2OConf()
hc <- H2OContext.getOrCreate(h2oConf)
```

# Create datacleaning pipeline

```{r}
# Create the main pipeline
# Remove unnecessary variables
# Remove any row with any NA value
# Classify flights with a delay with "1",  and no delay with "0"
main_pipeline <- .%>%
  na.omit() %>%
  mutate(delay_time = actual_arrival_time - planned_arrival_time,
         delay_time = minute(delay_time) + (hour(delay_time)*60) + (second(delay_time)/60),
         delay = case_when(delay_time > 0 ~ "1",
                           delay_time <= 0 ~ "0")) %>%
  select(quarter,month,day_of_month,day_of_week,hour_of_day,minutes_of_hour,
         planned_departure_local_hour,planned_arrival_local_hour,
         # wheels_off_time_local_hour,wheels_on_time_local_hour,
         flight_distance,seating_capacity,
         delay,delay_time)

# Create the numerical delay sub-pipeline
# Sum the individual types of delay to a total delay_time.
# Remove individual delays
numer_delay_pipeline <- .%>% main_pipeline %>% 
  select(-delay)

# Create the classification sub-pipeline
# Remove the individual delays 
class_delay_pipeline <- .%>% main_pipeline %>% 
  select(-delay_time) %>%
  group_by(delay)
```

# Splitting data in train and test sets

## Creating the train and test sets for spark

Take a sample set of the data to reduce time to train the model.

```{r, message=FALSE}
# Split for classification
classification_split <- create_train_test_split(data = class_delay_pipeline(raw_data),
                                                sample_size = 100000,
                                                ratio = 0.8,
                                                type = "classification")
train_data <- classification_split$train_data
test_data <- classification_split$test_data
# Split for numerical predictions
numerical_split <- create_train_test_split(data = numer_delay_pipeline(raw_data),
                                      sample_size = 100000,
                                      ratio = 0.8,
                                      type = "numerical")
train_data_regr <- numerical_split$train_data
test_data_regr <- numerical_split$test_data
```

Let's glimpse into the train data for classification.

```{r}
train_data %>% glimpse()
```

Let's glimpse into the train data for numerical prediction.

```{r}
train_data_regr %>% glimpse()
```

## Creating the train, validation, and test sets for h2o

Create the train, validation, test, split to use with the h2o framework.

```{r, message=FALSE}
tmp <- class_delay_pipeline(raw_data)

sub_sample <- 100000

delay_yes <- tmp %>%
  filter(delay == "1")
delay_no <- tmp %>%
  filter(delay == "0")
    
tmp1 <- delay_yes %>% 
      sample_n(sub_sample/2)
tmp2 <- delay_no %>% 
  sample_n(sub_sample/2)

sample_data_classification <- rbind(tmp1, tmp2)
sample_data_classification <- hc$asH2OFrame(sample_data_classification)
sample_data_numerical <- tmp %>%
  sample_n(100000)
sample_data_numerical <- hc$asH2OFrame(sample_data_numerical)
    
### Train, validation, test split using h2o framework with classification
splits <- h2o.splitFrame(
  data = sample_data_classification,
  ratios = c(0.6,0.2),   ## only need to specify 2 fractions, the 3rd is implied
  destination_frames = c("train1.hex", "valid1.hex", "test1.hex"), seed = 1234
)
train_data_h2o <- splits[[1]]
valid_data_h2o <- splits[[2]]
test_data_h2o  <- splits[[3]]

### Train, validation, test split using h2o framework with numerical prediction
splits <- h2o.splitFrame(
  data = sample_data_numerical,
  ratios = c(0.6,0.2),   ## only need to specify 2 fractions, the 3rd is implied
  destination_frames = c("train2.hex", "valid2.hex", "test2.hex"), seed = 1234
)
train_data_regr_h2o <- splits[[1]]
valid_data_regr_h2o <- splits[[2]]
test_data_regr_h2o  <- splits[[3]]

rm(raw_data)
```

# Classification of flight delays

In this section we will build and evaluate different machine learning models to predict if a given inbound flight in the United States will have a delay based on the data prepared in the previous sections.

## Logistic regression model

In this section we will build a logistic regression pipeline and cross-validate and hyper-parameter tune a logistic regression model.

### Building a logistic regression pipeline 

Below we build a ML pipeline for a logistic regression model to use cross validation as we perform hyper parameter tuning on our model.

```{r}
# Pipeline
glm_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_logistic_regression()

# Grid
grid <- list(logistic_regression = list(elastic_net_param = c(0,0.25,0.5,0.75,1), reg_param = c(0,0.25,0.5,0.75,1)))

# Cross validate model
glm_cv <- cross_validator(sc = sc,
                          data = train_data,
                          pipeline = glm_pipeline,
                          grid = grid,
                          type = "classification",
                          folds = 4,
                          seed = 2018)

# Get model results
a <- glm_cv$all_results
glm_cv_best_result <- glm_cv$best_result
glm_cv_result <- glm_cv_best_result[which.max(glm_cv_best_result$accuracy),"accuracy"]
glm_train_time <- glm_cv$train_time
a[order(-a$accuracy),] %>% 
  head(5) %>%
  kbl() %>%
  kable_minimal()
```

### Train a tuned logistic regression model

Train a logistic regression model using the full training data set and the parameters that rolled out of the cross validation with hyper model parameter tuning.

```{r}
### Train a logistic regression model ###
glm_tuned_start_time <- Sys.time()
glm_model <- ml_logistic_regression(train_data, "delay ~ .",
                                    elastic_net_param = glm_cv_best_result[which.max(glm_cv_best_result$accuracy),"elastic_net_param_1"],
                                    reg_param = glm_cv_best_result[which.max(glm_cv_best_result$accuracy),"reg_param_1"])
glm_tuned_end_time <- Sys.time()
glm_tuned_train_time <- glm_tuned_end_time - glm_tuned_start_time
### Performance on train set

glm_tuned_result_train <- generate_metrics_classification(model = glm_model,
                                                          type = "train")

glm_tuned_result_train %>% 
  kbl() %>%
  kable_minimal()
```

Save the model to be able to reuse it later on predictions.

```{r, message=FALSE}
ml_save(glm_model, "models/glm_model", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
glm_tuned_result <- generate_metrics_classification(glm_model,
                                                    "test",
                                                    test_data)

glm_tuned_result %>% 
  kbl() %>%
  kable_minimal()
```

## Decision tree model

In this section we will build a decision tree pipeline and cross-validate and hyper-parameter tune a decision tree model.

### Building a decision tree pipeline

```{r}
# Pipeline
tree_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_decision_tree_classifier()

# Grid
grid <- list(decision_tree = list(max_depth = c(1,3,5,7,10)))

# Cross validate model
tree_cv <- cross_validator(sc = sc,
                           data = train_data,
                           pipeline = tree_pipeline,
                           grid = grid,
                           type = "classification",
                           folds = 4,
                           seed = 2018)

# Get model results
a <- tree_cv$all_results
tree_cv_best_result <- tree_cv$best_result
tree_cv_result <- tree_cv_best_result[which.max(tree_cv_best_result$accuracy),"accuracy"]
tree_train_time <- tree_cv$train_time
a[order(-a$accuracy),] %>% 
  head(5) %>%
  kbl() %>%
  kable_minimal()
```

### Train a decision tree model

Train a decision tree model using the train dataset and get model performance on training data.

```{r}
### Train a decision tree model ###
tree_tuned_start_time <- Sys.time()
tree_model <- ml_decision_tree_classifier(train_data, 
                                          "delay ~ .",
                                          max_depth = tree_cv_best_result[which.max(tree_cv_best_result$accuracy),"max_depth_1"])
tree_tuned_end_time <- Sys.time()
tree_tuned_train_time <- tree_tuned_end_time - tree_tuned_start_time

### Performance on train set
tree_tuned_result_train <- generate_metrics_classification(tree_model,"train")

tree_tuned_result_train %>% 
  kbl() %>%
  kable_minimal()
```

Save the model to be able to reuse it later on predictions.

```{r, message=FALSE}
ml_save(tree_model, "models/tree_model_classification", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
tree_tuned_result <- generate_metrics_classification(tree_model,
                                                    "test",
                                                    test_data)

tree_tuned_result %>% 
  kbl() %>%
  kable_minimal()
```

## Random forest model

In this section we will build a random forest pipeline and cross-validate and hyper-parameter tune a random forest model.

### Building a random forest pipeline

```{r}
# Pipeline
rf_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_random_forest_classifier()

# Grid
grid <- list(random_forest = list(max_depth = c(1,3,5,7,10), num_trees = c(1,3,5,7,10,25,50)))

# Cross validate model
rf_cv <- cross_validator(sc = sc,
                         data = train_data,
                         pipeline = rf_pipeline,
                         grid = grid,
                         type = "classification",
                         folds = 4,
                         seed = 2018)

# Get model results
a <- rf_cv$all_results
rf_cv_best_result <- rf_cv$best_result
rf_cv_result <- rf_cv_best_result[which.max(rf_cv_best_result$accuracy),"accuracy"]
rf_train_time <- rf_cv$train_time
a[order(-a$accuracy),] %>% 
  head(5) %>%
  kbl() %>%
  kable_minimal()
```

### Train a random forest model

Train a random forest model using the train dataset and get model performance on training data.

```{r}
### Train a decision tree model ###
rf_tuned_start_time <- Sys.time()
rf_model <- ml_random_forest_classifier(train_data, 
                                        "delay ~ .", 
                                        num_trees = rf_cv_best_result[which.max(rf_cv_best_result$accuracy),"num_trees_1"],
                                        max_depth = rf_cv_best_result[which.max(rf_cv_best_result$accuracy),"max_depth_1"])
rf_tuned_end_time <- Sys.time()
rf_tuned_train_time <- rf_tuned_end_time - rf_tuned_start_time

### Performance on train set
rf_tuned_result_train <- generate_metrics_classification(rf_model,"train")

rf_tuned_result_train %>% 
  kbl() %>%
  kable_minimal()
```

Save the model to be able to reuse it later on predictions.

```{r, message=FALSE}
ml_save(rf_model, "models/rf_model_classification", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
rf_tuned_result <- generate_metrics_classification(rf_model,
                                                    "test",
                                                    test_data)

rf_tuned_result %>% 
  kbl() %>%
  kable_minimal()
```

## Training a gradient boosting machine with h2o

In this section we will train a gradient boosted machine model using the h2o framework. 

<!-- Check this link for the tutorial for hyper parameter tuning a GBM model: -->
<!-- https://h2o.ai/blog/h2o-gbm-tuning-tutorial-for-r/ -->

### Baseline performance

Let's train a baseline GBM model using the train data set.

```{r, message=FALSE, fig.show='hide', results='hide'}
# Establish baseline performance
y <- "delay"
x <- setdiff(names(train_data_h2o), y)

gbm <- h2o.gbm(x = x, y = y, training_frame = train_data_h2o)
```

Let's take a look at the baseline AUC using the validation data set.

```{r}
h2o.auc(h2o.performance(gbm, newdata = valid_data_h2o))
```


baseline performance with 4-fold cross validation.

```{r, message=FALSE, fig.show='hide', results='hide'}
gbm_cv <- h2o.gbm(x=x,
                  y=y,
                  training_frame = h2o.rbind(train_data_h2o, valid_data_h2o),
                  nfolds = 4,
                  seed = 2018)
```

Let's take a look at the baseline AUC with cross validation using both the train and validation data set.

```{r}
h2o.auc(h2o.performance(gbm_cv, xval = TRUE))
```

Training a GBM with arbitrarily picked parameters.

```{r, message=FALSE, fig.show='hide', results='hide'}
gbm <- h2o.gbm(
  ## standard model parameters
  x = x,
  y = y,
  training_frame = train_data_h2o,
  validation_frame = valid_data_h2o,
  ## more trees is better if the learning rate is small enough
  ## here, use "more than enough" trees - we have early stopping
  ntrees = 10000,
  ## smaller learning rate is better (this is a good value for most datasets, but see below for annealing)
  learn_rate=0.01,
  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
  stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "AUC",
  ## sample 80% of rows per tree
  sample_rate = 0.8,
  ## sample 80% of columns per split
  col_sample_rate = 0.8,
  ## fix a random number generator seed for reproducibility
  seed = 2018,
  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
  score_tree_interval = 10
)
```

Get the AUC on the validation data set using some arbitrarily picked parameters.

```{r}
## Get the AUC on the validation set
h2o.auc(h2o.performance(gbm))
```

### Hyper-parameter tuning the GBM model

Perform hyper-parameter tuning to figure out the best value for depth.

```{r, message=FALSE, fig.show='hide', results='hide'}
## Depth 10 is usually plenty of depth for most datasets, but you never know
hyper_params = list( max_depth = seq(1,29,2) )
#hyper_params = list( max_depth = c(4,6,8,12,16,20) ) ##faster for larger datasets
grid <- h2o.grid(
  ## hyper parameters
  hyper_params = hyper_params,
  ## full Cartesian hyper-parameter search
  search_criteria = list(strategy = "Cartesian"),
  ## which algorithm to run
  algorithm="gbm",
  ## identifier for the grid, to later retrieve it
  grid_id="depth_grid",
  ## standard model parameters
  x = x,
  y = y,
  training_frame = train_data_h2o,
  validation_frame = valid_data_h2o,
  ## more trees is better if the learning rate is small enough
  ## here, use "more than enough" trees - we have early stopping
  ntrees = 10000,
  ## smaller learning rate is better
  ## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
  learn_rate = 0.05,
  ## learning rate annealing: learning_rate shrinks by 1% after every tree
  ## (use 1.00 to disable, but then lower the learning_rate)
  learn_rate_annealing = 0.99,
  ## sample 80% of rows per tree
  sample_rate = 0.8,
  ## sample 80% of columns per split
  col_sample_rate = 0.8,
  ## fix a random number generator seed for reproducibility
  seed = 1234,
  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
  stopping_rounds = 5,
  stopping_tolerance = 1e-4,
  stopping_metric = "AUC",
  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
  score_tree_interval = 10
)
```

```{r}
## by default, display the grid search results sorted by increasing logloss (since this is a classification task)
grid
```

```{r}
## sort the grid models by decreasing AUC
sortedGrid <- h2o.getGrid("depth_grid", sort_by="auc", decreasing = TRUE)
sortedGrid
```

```{r}
## find the range of max_depth for the top 5 models
topDepths = sortedGrid@summary_table$max_depth[1:5]
minDepth = min(as.numeric(topDepths))
maxDepth = max(as.numeric(topDepths))
```

Hyper parameter tune the remaining parameters using random search.

```{r, message=FALSE, fig.show='hide', results='hide'}
hyper_params = list(
  ## restrict the search to the range of max_depth established above
  max_depth = seq(minDepth,maxDepth,1),
  ## search a large space of row sampling rates per tree
  sample_rate = seq(0.2,1,0.01),
  ## search a large space of column sampling rates per split
  col_sample_rate = seq(0.2,1,0.01),
  ## search a large space of column sampling rates per tree
  col_sample_rate_per_tree = seq(0.2,1,0.01),
  ## search a large space of how column sampling per split should change as a function of the depth of the split
  col_sample_rate_change_per_level = seq(0.9,1.1,0.01),
  ## search a large space of the number of min rows in a terminal node
  min_rows = 2^seq(0,log2(nrow(train_data_h2o))-1,1),
  ## search a large space of the number of bins for split-finding for continuous and integer columns
  nbins = 2^seq(4,10,1),
  ## search a large space of the number of bins for split-finding for categorical columns
  nbins_cats = 2^seq(4,12,1),
  ## search a few minimum required relative error improvement thresholds for a split to happen
  min_split_improvement = c(0,1e-8,1e-6,1e-4),
  ## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
  histogram_type = c("UniformAdaptive","QuantilesGlobal","RoundRobin")
)
search_criteria = list(
  ## Random grid search
  strategy = "RandomDiscrete",
  ## limit the runtime to 60 minutes
  max_runtime_secs = 3600,
  ## build no more than 100 models
  max_models = 100,
  ## random number generator seed to make sampling of parameter combinations reproducible
  seed = 1234,
  ## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
  stopping_rounds = 5,
  stopping_metric = "AUC",
  stopping_tolerance = 1e-3
)
grid <- h2o.grid(
  ## hyper parameters
  hyper_params = hyper_params,
  ## hyper-parameter search configuration (see above)
  search_criteria = search_criteria,
  ## which algorithm to run
  algorithm = "gbm",
  ## identifier for the grid, to later retrieve it
  grid_id = "final_grid",
  ## standard model parameters
  x = x,
  y = y,
  training_frame = train_data_h2o,
  validation_frame = valid_data_h2o,
  ## more trees is better if the learning rate is small enough
  ## use "more than enough" trees - we have early stopping
  ntrees = 10000,
  ## smaller learning rate is better
  ## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
  learn_rate = 0.05,
  ## learning rate annealing: learning_rate shrinks by 1% after every tree
  ## (use 1.00 to disable, but then lower the learning_rate)
  learn_rate_annealing = 0.99,
  ## early stopping based on timeout (no model should take more than 1 hour - modify as needed)
  max_runtime_secs = 3600,
  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
  stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "AUC",
  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
  score_tree_interval = 10,
  ## base random number generator seed for each model (automatically gets incremented internally for each model)
  seed = 1234
)
```

```{r}
## Sort the grid models by AUC
sortedGrid <- h2o.getGrid("final_grid", sort_by = "auc", decreasing = TRUE)
sortedGrid
```

Inspection of the best model of the grid search:

```{r}
gbm <- h2o.getModel(sortedGrid@model_ids[[1]])
print(h2o.auc(h2o.performance(gbm, newdata = test_data_h2o)))
```

Check the parameters of the winning model.

```{r}
gbm@parameters
```

### Train the tuned model using the whole training set

Build a model on the whole training set:

```{r, message=FALSE, fig.show='hide', results='hide'}
final_gbm_model <- do.call(h2o.gbm,
        ## update parameters in place
        {
          p <- gbm@parameters
          p$model_id = NULL          ## do not overwrite the original grid model
          p$training_frame = h2o.rbind(train_data_h2o, valid_data_h2o)      ## use the full dataset
          p$validation_frame = NULL  ## no validation frame
          p$nfolds = 4               ## cross-validation
          p
        }
)
```

```{r}
final_gbm_model@model$cross_validation_metrics_summary
```

<!-- You can use the following code to check the variance: -->
<!-- Commenting out since it does not show much effect. -->

<!-- ```{r} -->
<!-- for (i in 1:5) { -->
<!--   gbm <- h2o.getModel(sortedGrid@model_ids[[i]]) -->
<!--   cvgbm <- do.call(h2o.gbm, -->
<!--         ## update parameters in place -->
<!--         { -->
<!--           p <- gbm@parameters -->
<!--           p$model_id = NULL          ## do not overwrite the original grid model -->
<!--           p$training_frame = h2o.rbind(train_data_h2o, valid_data_h2o)      ## use the full dataset -->
<!--           p$validation_frame = NULL  ## no validation frame -->
<!--           p$nfolds = 5               ## cross-validation -->
<!--           p -->
<!--         } -->
<!--   ) -->
<!--   print(gbm@model_id) -->
<!--   print(cvgbm@model$cross_validation_metrics_summary[5,]) ## Pick out the "AUC" row -->
<!-- } -->
<!-- ``` -->

Make predictions on the test set:

```{r, message=FALSE, fig.show='hide', results='hide'}
gbm <- h2o.getModel(sortedGrid@model_ids[[1]])
preds <- h2o.predict(gbm, test_data_h2o)
```

Show some of the predictions:

```{r}
head(preds)
```

Show the metrics of the model with the thresholds

```{r}
gbm@model$validation_metrics@metrics$max_criteria_and_metric_scores
```

Save the model:

```{r, message=FALSE, fig.show='hide', results='hide'}
h2o.saveModel(gbm, "models/h2o/gbm_classification.csv", force=TRUE)
```

## Train a deep learning model using the h2o framework

### Baseline performance

Establish baseline performance

```{r, message=FALSE, fig.show='hide', results='hide'}
dl_model <- h2o.deeplearning(
  model_id = "dl_model1", 
  training_frame = train_data_h2o, 
  validation_frame = valid_data_h2o,   ## validation dataset: used for scoring and early stopping
  x = x,
  y = y,
  #activation="Rectifier",  ## default
  #hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each
  epochs = 1,
  variable_importances = T    ## not enabled by default
)
# summary(dl_model)
```

Let's take a look at variable importance

```{r}
head(as.data.frame(h2o.varimp(dl_model))) %>%
  kbl() %>%
  kable_minimal()
```

Implement early stopping to the model

```{r, message=FALSE, fig.show='hide', results='hide'}
m2 <- h2o.deeplearning(
  model_id="dl_model_faster", 
  training_frame=train_data_h2o, 
  validation_frame=valid_data_h2o,
  x=x,
  y=y,
  hidden=c(32,32,32),                  ## small network, runs faster
  epochs=1000000,                      ## hopefully converges earlier...
  score_validation_samples=10000,      ## sample the validation dataset (faster)
  stopping_rounds=2,
  stopping_metric="misclassification", ## could be "MSE","logloss","r2"
  stopping_tolerance=0.01
)
# summary(m2)
# plot(m2)
```

### Hyper parameter tuning the deep learning model

Tuning the deep learning model

```{r, message=FALSE, fig.show='hide', results='hide'}
m3 <- h2o.deeplearning(
  model_id="dl_model_tuned", 
  training_frame=train_data_h2o, 
  validation_frame=valid_data_h2o, 
  x=x, 
  y=y, 
  overwrite_with_best_model=F,    ## Return the final model after 10 epochs, even if not the best
  hidden=c(128,128,128),          ## more hidden layers -> more complex interactions
  epochs=10,                      ## to keep it short enough
  score_validation_samples=10000, ## downsample validation set for faster scoring
  score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time
  adaptive_rate=F,                ## manually tuned learning rate
  rate=0.01, 
  rate_annealing=2e-6,            
  momentum_start=0.2,             ## manually tuned momentum
  momentum_stable=0.4, 
  momentum_ramp=1e7, 
  l1=1e-5,                        ## add some L1/L2 regularization
  l2=1e-5,
  max_w2=10                       ## helps stability for Rectifier
) 
summary(m3)
```

Hyper parameter tuning with grid search

```{r, message=FALSE, fig.show='hide', results='hide'}
hyper_params <- list(
  hidden=list(c(32,32,32),c(64,64)),
  input_dropout_ratio=c(0,0.05),
  rate=c(0.01,0.02),
  rate_annealing=c(1e-8,1e-7,1e-6)
)
hyper_params
grid <- h2o.grid(
  algorithm="deeplearning",
  grid_id="dl_grid", 
  training_frame=train_data_h2o,
  validation_frame=valid_data_h2o, 
  x=x, 
  y=y,
  epochs=10,
  stopping_metric="misclassification",
  stopping_tolerance=1e-2,        ## stop when misclassification does not improve by >=1% for 2 scoring events
  stopping_rounds=2,
  score_validation_samples=10000, ## downsample validation set for faster scoring
  score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time
  adaptive_rate=F,                ## manually tuned learning rate
  momentum_start=0.5,             ## manually tuned momentum
  momentum_stable=0.9, 
  momentum_ramp=1e7, 
  l1=1e-5,
  l2=1e-5,
  activation=c("Rectifier"),
  max_w2=10,                      ## can help improve stability for Rectifier
  hyper_params=hyper_params
)
```

```{r}
## Sort the grid models by AUC
sortedGrid <- h2o.getGrid("dl_grid", sort_by = "auc", decreasing = TRUE)
sortedGrid
```

```{r, message=FALSE, fig.show='hide', results='hide'}
best_model <- h2o.getModel(grid@model_ids[[1]])
```

### Train a tuned deep learning model using the entire training set

Build a model on the whole training set:

```{r, message=FALSE, fig.show='hide', results='hide'}
final_dl_model <- do.call(h2o.deeplearning,
        ## update parameters in place
        {
          p <- best_model@parameters
          p$model_id = NULL          ## do not overwrite the original grid model
          p$training_frame = h2o.rbind(train_data_h2o, valid_data_h2o)      ## use the full dataset
          p$validation_frame = NULL  ## no validation frame
          p$nfolds = 4               ## cross-validation
          p
        }
)
```

```{r}
final_dl_model@model$cross_validation_metrics_summary
```

Make predictions on the test set:

```{r, message=FALSE, fig.show='hide', results='hide'}
dl_model <- h2o.getModel(sortedGrid@model_ids[[1]])
preds_dl <- h2o.predict(dl_model, test_data_h2o)
```

Save the model:

```{r, message=FALSE, fig.show='hide', results='hide'}
h2o.saveModel(dl_model, "models/h2o/dl_classification.csv", force=TRUE)
```

# Numerical prediction of the delay time

In this section we will build and evaluate different machine learning models to predict the amount of delay an inbound flight of the United States will have.

## Random Forest Model

In this section we will build a random forest pipeline and cross-validate and hyper-parameter tune a random forest model for the numerical delay prediction.

### Building a random forest pipeline

```{r}
# Pipeline
rf_pipeline_regression <- ml_pipeline(sc) %>%
    ft_r_formula(delay_time ~ .) %>%
    ml_random_forest_regressor()

# Grid
grid <- list(random_forest = list(max_depth = c(1,3,5,7,10), num_trees = c(1,3,5,7,10,25,50)))

# Cross validate model
rf_cv <- cross_validator(sc = sc,
                         data = train_data_regr,
                         pipeline = rf_pipeline_regression,
                         grid = grid,
                         type = "numerical",
                         folds = 4,
                         seed = 2018)

# Get model results
a <- rf_cv$all_results
rf_cv_best_result_regr <- rf_cv$best_result 
rf_cv_result_regr <- rf_cv_best_result_regr[which.max(rf_cv_best_result_regr$r2),"r2"]
rf_train_time_regr <- rf_cv$train_time
a[order(-a$r2),] %>% 
  head(5) %>%
  kbl() %>%
  kable_minimal()
```

### Train a random forest model

Train a random forest model using the train dataset and get model performance on training data.

```{r}
### Train a random forest model ###
rf_regr_tuned_start_time <- Sys.time()
rf_model_regr <- ml_random_forest_regressor(train_data_regr,
                                        "delay_time ~ .",
                                        num_trees = rf_cv_best_result_regr[which.max(rf_cv_best_result_regr$r2),"num_trees_1"],
                                        max_depth = rf_cv_best_result_regr[which.max(rf_cv_best_result_regr$r2),"max_depth_1"])
rf_regr_tuned_end_time <- Sys.time()
rf_regr_tuned_train_time <- rf_regr_tuned_end_time - rf_regr_tuned_start_time

### Performance on train set
augment_value <- augment(rf_model_regr) %>% collect()
metric_flights <- metric_set(rsq, mae, rmse)

rf_regr_tuned_result_train <- metric_flights(augment_value, truth = delay_time, estimate = .prediction)

rf_regr_tuned_result_train %>% 
  kbl() %>%
  kable_minimal()
```

Save the model to be able to reuse it later on predictions.

```{r, message=FALSE}
ml_save(rf_model_regr, "models/rf_model_regression", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
rf_regr_tuned_result_test <- ml_predict(rf_model_regr,test_data_regr) %>%
  ml_metrics_regression(truth = delay_time, estimate = prediction)

rf_regr_tuned_result_test %>% 
  kbl() %>%
  kable_minimal()
```

# Summary of results

## Cross validation

### Classification

```{r}
model_type <- c("glm", "decision tree", "random forest")
train_time <- c(glm_train_time, tree_train_time, rf_train_time)
cv_results <- c(glm_cv_result, tree_cv_result, rf_cv_result)

classification_results <- data.frame(model_type, train_time, cv_results)
colnames(classification_results) <- c("model type", "train time", "cv accuracy")
classification_results %>% 
  kbl() %>%
  kable_minimal()
```

### Numerical prediction

```{r}
model_type <- c("random forest")
train_time <- c(rf_train_time_regr)
cv_results <- c(rf_cv_result_regr)

numerical_prediction_results <- data.frame(model_type, train_time, cv_results)
colnames(numerical_prediction_results) <- c("model type", "train time", "cv r-squared")
numerical_prediction_results %>% 
  kbl() %>%
  kable_minimal()
```

## Final tuned models

### Classification

#### Models build using spark

Summary of model metrics on the train set

```{r, echo=FALSE}
model_type <- c("glm", "decision tree", "random forest")
train_time <- round(c(glm_tuned_train_time, tree_tuned_train_time, rf_tuned_train_time),3)

accuracy_results_train <- c(glm_tuned_result_train[1,]$value, tree_tuned_result_train[1,]$value, rf_tuned_result_train[1,]$value)
recall_results_train <- c(glm_tuned_result_train[2,]$value, tree_tuned_result_train[2,]$value, rf_tuned_result_train[2,]$value)
precision_results_train <- c(glm_tuned_result_train[3,]$value, tree_tuned_result_train[3,]$value, rf_tuned_result_train[3,]$value)
missclassification_results_train<- c(glm_tuned_result_train[4,]$value, tree_tuned_result_train[4,]$value, rf_tuned_result_train[4,]$value)
specificity_results_train <- c(glm_tuned_result_train[5,]$value, tree_tuned_result_train[5,]$value, rf_tuned_result_train[5,]$value)
f1_results_train <- c(glm_tuned_result_train[6,]$value, tree_tuned_result_train[6,]$value, rf_tuned_result_train[6,]$value)
roc_auc_results_train <- c(glm_tuned_result_train[7,]$value, tree_tuned_result_train[7,]$value, rf_tuned_result_train[7,]$value)
pr_auc_results_train <- c(glm_tuned_result_train[8,]$value, tree_tuned_result_train[8,]$value, rf_tuned_result_train[8,]$value)

classification_results_train <- data.frame(model_type, 
                                     train_time, 
                                     accuracy_results_train, 
                                     recall_results_train,
                                     precision_results_train,
                                     missclassification_results_train,
                                     specificity_results_train,
                                     f1_results_train,
                                     roc_auc_results_train,
                                     pr_auc_results_train)

colnames(classification_results_train) <- c("model type", 
                                      "train time", 
                                      "accuracy", 
                                      "recall",
                                      "precision",
                                      "missclassification",
                                      "specificity",
                                      "f1",
                                      "roc_auc",
                                      "pr_auc")
classification_results_train %>% 
  kbl() %>%
  kable_minimal()
```

Summary of model metrics on the test set

```{r, echo=FALSE}
accuracy_results_test <- c(glm_tuned_result[1,]$value, tree_tuned_result[1,]$value, rf_tuned_result[1,]$value)
recall_results_test <- c(glm_tuned_result[2,]$value, tree_tuned_result[2,]$value, rf_tuned_result[2,]$value)
precision_results_test <- c(glm_tuned_result[3,]$value, tree_tuned_result[3,]$value, rf_tuned_result[3,]$value)
missclassification_results_test <- c(glm_tuned_result[4,]$value, tree_tuned_result[4,]$value, rf_tuned_result[4,]$value)
specificity_results_test <- c(glm_tuned_result[5,]$value, tree_tuned_result[5,]$value, rf_tuned_result[5,]$value)
f1_results_test <- c(glm_tuned_result[6,]$value, tree_tuned_result[6,]$value, rf_tuned_result[6,]$value)
roc_auc_results_test <- c(glm_tuned_result[7,]$value, tree_tuned_result[7,]$value, rf_tuned_result[7,]$value)
pr_auc_results_test <- c(glm_tuned_result[8,]$value, tree_tuned_result[8,]$value, rf_tuned_result[8,]$value)

classification_results_train <- data.frame(model_type, 
                                     accuracy_results_test,
                                     recall_results_test,
                                     precision_results_test,
                                     missclassification_results_test,
                                     specificity_results_test,
                                     f1_results_test,
                                     roc_auc_results_test,
                                     pr_auc_results_test)
colnames(classification_results_train) <- c("model",
                                      "accuracy", 
                                      "recall",
                                      "precision",
                                      "missclassification",
                                      "specificity",
                                      "f1",
                                      "roc_auc",
                                      "pr_auc")
classification_results_train %>% 
  kbl() %>%
  kable_minimal()
```

#### Models build using h2o

```{r, echo=FALSE, message=FALSE, fig.show='hide', results='hide'}
gbm_metrics_train <- generate_metrics_classification(model = final_gbm_model,
                                                    type = "h2o_classification_train")
colnames(gbm_metrics_train) <- c("metric_names", "GBM_train")
```

```{r, echo=FALSE, message=FALSE, fig.show='hide', results='hide'}
gbm_metrics_test <- generate_metrics_classification(model = gbm,
                                                    type = "h2o_classification_test",
                                                    test_data = test_data_h2o,
                                                    sc = sc)
```

```{r, echo=FALSE, message=FALSE, fig.show='hide', results='hide'}
dl_metrics_train <- generate_metrics_classification(model = final_dl_model,
                                                    type = "h2o_classification_train")
colnames(dl_metrics_train) <- c("metric_names", "DL_train")
```


```{r, echo=FALSE, message=FALSE, fig.show='hide', results='hide'}
dl_metrics_test <- generate_metrics_classification(model = dl_model,
                                                    type = "h2o_classification_test",
                                                    test_data = test_data_h2o,
                                                    sc = sc)
```

Model metrics on the entire train set for the models trained using the h2o framework:

```{r, echo=FALSE}
final_summary_train <- gbm_metrics_train %>%
  append(data.frame(dl_metrics_train$DL_train))

final_summary_train <- data.frame(final_summary_train)
colnames(final_summary_train) <- c("metric_names", "GBM_train", "DL_train")

final_summary_train %>% 
  kbl() %>%
  kable_minimal()
```

Model metrics on the test set for the models trained using the h2o framework:

```{r, echo=FALSE}
final_summary_test <- gbm_metrics_test %>%
  append(data.frame(dl_metrics_test$value))


final_summary_test <- data.frame(final_summary_test)

colnames(final_summary_test) <- c("metric_names", "GBM_test", "DL_test")

final_summary_test %>% 
  kbl() %>%
  kable_minimal()
```

### Numerical prediction

Summary of model metrics on the train set

```{r, echo=FALSE}
model_type <- c("random forest")
train_time <- c(rf_regr_tuned_train_time)
r2_results_train <- c(rf_regr_tuned_result_train[1,]$.estimate)
mae_results_train <- c(rf_regr_tuned_result_train[2,]$.estimate)
rmse_results_train <- c(rf_regr_tuned_result_train[3,]$.estimate)

numerical_prediction_results <- data.frame(model_type, train_time, r2_results_train, mae_results_train, rmse_results_train)
colnames(numerical_prediction_results) <- c("model type", "train time", "R2", "MAE", "RMSE")

numerical_prediction_results %>% 
  kbl() %>%
  kable_minimal()
```

Summary of model metrics on the test set

```{r, echo=FALSE}
r2_results_test <- c(rf_regr_tuned_result_test[2,]$.estimate)
mae_results_test <- c(rf_regr_tuned_result_test[3,]$.estimate)
rmse_results_test <- c(rf_regr_tuned_result_test[1,]$.estimate)

numerical_prediction_results <- data.frame(model_type, train_time, r2_results_test, mae_results_test, rmse_results_test)
colnames(numerical_prediction_results) <- c("model type", "train time", "R2", "MAE", "RMSE")

numerical_prediction_results %>% 
  kbl() %>%
  kable_minimal()
markdown_end_time <- Sys.time()
```

The total time to knit this markdown is:

```{r}
markdown_end_time - markdown_start_time
```