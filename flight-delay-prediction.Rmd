---
title: "Study of delay prediction in the US airport network"
author: "Kerim Kili√ß"
subtitle: "Supervised Machined Learning using flight data"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 2
    number_sections: true
    toc_float: true
---

# Libraries

The following three libraries are used in this R markdown file.

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidymodels)
library(sparklyr)
```

# Initialize spark and read in data

```{r}
### Uncomment these two lines to install spark, 
### and optionally to disconnect an existing connection
# spark_install(version = "3.3.0")
# spark_disconnect(sc)
spark_config <- spark_config()
spark_config$'sparklyr.shell.driver-memory' <- "12G"

sc <- spark_connect(master = "local",
                    config = spark_config)

### Uncomment these two lines to read in the RDS file of the data of 2017
# my_data <- readRDS("data/flights_2017.RDS")
# fwrite(my_data,file = "data/flights_2017.csv")
raw_data <- spark_read_csv(sc,"data/flights_2017.csv")
raw_data %>% glimpse()
```

# Create datacleaning pipeline

```{r}
# Create the main pipeline
# Remove unnecessary variables
# Remove any row with any NA value
# Classify flights with a delay with "1",  and no delay with "0"
main_pipeline <- .%>%
  select(-id,-year,-wheels_on_time,-wheels_off_time,-planned_departure_time,
         -planned_arrival_time,-actual_departure_time,-actual_arrival_time,
         -tail_number,-flight_number,-carrier,-origin,-destination,
         -cancelled_flag,-diverted_flag,-num_flights) %>%
  na.omit() %>%
  mutate(delay = case_when(carrier_delay + weather_delay + nas_delay + security_delay + late_aircraft_delay > 0 ~ "1",
                           carrier_delay + weather_delay + nas_delay + security_delay + late_aircraft_delay == 0 ~ "0"))

# Create the numerical delay sub-pipeline
# Sum the individual types of delay to a total delay_time.
# Remove individual delays
numer_delay_pipeline <- .%>% main_pipeline %>% 
  mutate(delay_time = carrier_delay + weather_delay + nas_delay + security_delay + late_aircraft_delay) %>%
  select(-carrier_delay,-weather_delay,-nas_delay,-security_delay,-late_aircraft_delay,-delay)

# Create the classification sub-pipeline
# Remove the individual delays 
class_delay_pipeline <- .%>% main_pipeline %>% 
  select(-carrier_delay,-weather_delay,-nas_delay,-security_delay,-late_aircraft_delay)
```

# Splitting data in train and test sets

Take a sample set of the data to reduce time to train the model.

```{r}
## Splitted datasets for classification
train_data <- class_delay_pipeline(raw_data) %>%
  group_by(delay) %>%
  sample_frac(0.008)
test_data <- class_delay_pipeline(raw_data) %>%
  group_by(delay) %>%
  sample_frac(0.002)

## Splitted datasets for numerical prediction
train_data_regr <- numer_delay_pipeline(raw_data) %>%
  sample_frac(0.008)
test_data_regr <- numer_delay_pipeline(raw_data) %>%
  sample_frac(0.002)
```

# Classification of flight delays

In this section we will build and evaluate different machine learning models to predict if a given inbound flight in the United States will have a delay based on the data prepared in the previous sections.

## Logistic regression model

In this section we will build a logistic regression pipeline and cross-validate and hyper-parameter tune a logistic regression model.

### Building a logistic regression pipeline 

Below we build a ML pipeline for a logistic regression model to use cross validation as we perform hyper parameter tuning on our model.

```{r}
# Pipeline
glm_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_logistic_regression()

# Grid
grid <- list(logistic_regression = list(elastic_net_param = c(0,0.25,0.5,0.75,1), reg_param = c(0,0.25,0.5,0.75,1)))

# Cross validation
cv <- ml_cross_validator(
  sc,
  estimator = glm_pipeline, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_multiclass_classification_evaluator(sc,metric_name = "accuracy"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
)

cv_model <- ml_fit(cv, train_data)

a <- cv_model$avg_metrics_df
```


### Train a tuned logistic regression model

Train a logistic regression model using the full training data set and the parameters that rolled out of the cross validation with hyper model parameter tuning.

```{r}
### Train a logistic regression model ###
glm_model <- ml_logistic_regression(train_data, "delay ~ .",
                                    elastic_net_param = a[which.max(a$accuracy),"elastic_net_param_1"],
                                    reg_param = a[which.max(a$accuracy),"reg_param_1"])
### Performance on train set
augment_value <- augment(glm_model) %>% collect()
augment_value$delay <- as.factor(augment_value$delay)
augment_value$.predicted_label <- as.factor(augment_value$.predicted_label)
augment_value %>%
  accuracy(estimate = .predicted_label, truth = delay)

ml_save(glm_model, "models/glm_model", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
ml_predict(glm_model,test_data) %>%
  ml_metrics_multiclass()
```

## Decision tree model

In this section we will build a decision tree pipeline and cross-validate and hyper-parameter tune a decision tree model.

### Building a decision tree pipeline

```{r}
# Pipeline
tree_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_decision_tree_classifier()

# Grid
grid <- list(decision_tree = list(max_depth = c(1,3,5,7,10)))

# Cross validation
cv <- ml_cross_validator(
  sc,
  estimator = tree_pipeline, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_multiclass_classification_evaluator(sc,metric_name = "accuracy"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
)

cv_model <- ml_fit(cv, train_data)

a <- cv_model$avg_metrics_df
```


### Train a decision tree model

Train a decision tree model using the train dataset and get model performance on training data.

```{r}
### Train a decision tree model ###
tree_model <- ml_decision_tree_classifier(train_data, 
                                          "delay ~ .",
                                          max_depth = a[which.max(a$accuracy),"max_depth_1"])

### Performance on train set
augment_value <- augment(tree_model) %>% collect()
augment_value$delay <- as.factor(augment_value$delay)
augment_value$.predicted_label <- as.factor(augment_value$.predicted_label)
augment_value %>%
  accuracy(estimate = .predicted_label, truth = delay)

ml_save(tree_model, "models/tree_model_classification", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
ml_predict(tree_model,test_data) %>%
  ml_metrics_multiclass()
```

## Random forest model

In this section we will build a random forest pipeline and cross-validate and hyper-parameter tune a random forest model.

### Building a random forest pipeline

```{r}
# Pipeline
rf_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_random_forest_classifier()

# Grid
grid <- list(random_forest = list(max_depth = c(1,3,5,7,10), num_trees = c(1,3,5,7,10,25,50)))

# Cross validation
cv <- ml_cross_validator(
  sc,
  estimator = rf_pipeline, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_multiclass_classification_evaluator(sc,metric_name = "accuracy"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
)

cv_model <- ml_fit(cv, train_data)

a <- cv_model$avg_metrics_df
```

### Train a random forest model

Train a random forest model using the train dataset and get model performance on training data.

```{r}
### Train a decision tree model ###
rf_model <- ml_random_forest_classifier(train_data, 
                                        "delay ~ .", 
                                        num_trees = a[which.max(a$accuracy),"num_trees_1"],
                                        max_depth = a[which.max(a$accuracy),"max_depth_1"])

### Performance on train set
augment_value <- augment(rf_model) %>% collect()
augment_value$delay <- as.factor(augment_value$delay)
augment_value$.predicted_label <- as.factor(augment_value$.predicted_label)
augment_value %>%
  accuracy(estimate = .predicted_label, truth = delay)

ml_save(rf_model, "models/rf_model_classification", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
ml_predict(rf_model,test_data) %>%
  ml_metrics_multiclass()
```

# Numerical prediction of the delay time

In this section we will build and evaluate different machine learning models to predict the amount of delay an inbound flight of the United States will have.

## Random Forest Model

In this section we will build a random forest pipeline and cross-validate and hyper-parameter tune a random forest model for the numerical delay prediction.

### Building a random forest pipeline

```{r}
# Pipeline
rf_pipeline_regression <- ml_pipeline(sc) %>%
  ft_r_formula(delay_time ~ .) %>%
  ml_random_forest_regressor()

# Grid
grid <- list(random_forest = list(max_depth = c(1,3,5,7,10), num_trees = c(1,3,5,7,10,25,50)))

cv <- ml_cross_validator(
  sc,
  estimator = rf_pipeline_regression, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_regression_evaluator(sc,metric_name = "r2"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
 )

cv_model <- ml_fit(cv, train_data_regr)

a <- cv_model$avg_metrics_df

a[which.max(a$r2),]
```

### Train a random forest model

Train a random forest model using the train dataset and get model performance on training data.

```{r}
### Train a random forest model ###
rf_model_regr <- ml_random_forest_regressor(train_data_regr,
                                        "delay_time ~ .",
                                        num_trees = a[which.max(a$r2),"num_trees_1"],
                                        max_depth = a[which.max(a$r2),"max_depth_1"])

### Performance on train set
augment_value <- augment(rf_model_regr) %>% collect()
metric_flights <- metric_set(rsq, mae, rmse)
metric_flights(augment_value, truth = delay_time, estimate = .prediction)
ml_save(rf_model_regr, "models/rf_model_regression", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
ml_predict(rf_model_regr,test_data_regr) %>%
  ml_metrics_regression(truth = delay_time, estimate = prediction)
```
