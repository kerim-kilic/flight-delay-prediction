---
title: "Study of delay prediction in the US airport network"
author: "Kerim Kili√ß"
subtitle: "Supervised Machined Learning using flight data"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 2
    number_sections: true
    toc_float: true
---

# Libraries

The following three libraries are used in this R markdown file.

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidymodels)
library(sparklyr)
```

# Initialize spark and read in data

```{r}
### Uncomment these two lines to install spark, 
### and optionally to disconnect an existing connection
# spark_install(version = "3.3.0")
# spark_disconnect(sc)
spark_config <- spark_config()
spark_config$'sparklyr.shell.driver-memory' <- "12G"

sc <- spark_connect(master = "local",
                    config = spark_config)

### Uncomment these two lines to read in the RDS file of the data of 2017
# my_data <- readRDS("data/flights_2017.RDS")
# fwrite(my_data,file = "data/flights_2017.csv")
raw_data <- spark_read_csv(sc,"data/flights_2017.csv")
raw_data %>% glimpse()
```

# Create datacleaning pipeline

```{r}
# Create the main pipeline
# Remove unnecessary variables
# Remove any row with any NA value
# Classify flights with a delay with "1",  and no delay with "0"
main_pipeline <- .%>%
  select(-id,-year,-wheels_on_time,-wheels_off_time,-planned_departure_time,
         -planned_arrival_time,-actual_departure_time,-actual_arrival_time,
         -tail_number,-flight_number,-carrier,-origin,-destination,
         -cancelled_flag,-diverted_flag,-num_flights,
         -actual_arrival_local_hour, -actual_departure_local_hour) %>%
  na.omit() %>%
  mutate(delay = case_when(carrier_delay + weather_delay + nas_delay + security_delay + late_aircraft_delay > 0 ~ "1",
                           carrier_delay + weather_delay + nas_delay + security_delay + late_aircraft_delay == 0 ~ "0"))

# Create the numerical delay sub-pipeline
# Sum the individual types of delay to a total delay_time.
# Remove individual delays
numer_delay_pipeline <- .%>% main_pipeline %>% 
  mutate(delay_time = carrier_delay + weather_delay + nas_delay + security_delay + late_aircraft_delay) %>%
  select(-carrier_delay,-weather_delay,-nas_delay,-security_delay,-late_aircraft_delay,-delay)

# Create the classification sub-pipeline
# Remove the individual delays 
class_delay_pipeline <- .%>% main_pipeline %>% 
  select(-carrier_delay,-weather_delay,-nas_delay,-security_delay,-late_aircraft_delay) %>%
  group_by(delay)

# class_delay_pipeline(raw_data) %>% glimpse()
# numer_delay_pipeline(raw_data) %>% glimpse()
```

# Splitting data in train and test sets

Take a sample set of the data to reduce time to train the model.

```{r}
## Splitted datasets for classification
train_data <- class_delay_pipeline(raw_data) %>%
  sample_frac(0.008)
test_data <- class_delay_pipeline(raw_data) %>%
  sample_frac(0.002)

## Splitted datasets for numerical prediction
train_data_regr <- numer_delay_pipeline(raw_data) %>%
  sample_frac(0.008)
test_data_regr <- numer_delay_pipeline(raw_data) %>%
  sample_frac(0.002)
```

# Classification of flight delays

In this section we will build and evaluate different machine learning models to predict if a given inbound flight in the United States will have a delay based on the data prepared in the previous sections.

## Logistic regression model

In this section we will build a logistic regression pipeline and cross-validate and hyper-parameter tune a logistic regression model.

### Building a logistic regression pipeline 

Below we build a ML pipeline for a logistic regression model to use cross validation as we perform hyper parameter tuning on our model.

```{r}
# Pipeline
glm_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_logistic_regression()

# Grid
grid <- list(logistic_regression = list(elastic_net_param = c(0,0.25,0.5,0.75,1), reg_param = c(0,0.25,0.5,0.75,1)))

# Cross validation
cv <- ml_cross_validator(
  sc,
  estimator = glm_pipeline, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_multiclass_classification_evaluator(sc,metric_name = "accuracy"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
)

glm_start_time <- Sys.time()
cv_model <- ml_fit(cv, train_data)
glm_end_time <- Sys.time()
# Measure the time it takes to cross validate glm model
glm_train_time <- glm_end_time - glm_start_time

a <- cv_model$avg_metrics_df
glm_cv_result <- a[which.max(a$accuracy),"accuracy"]
```


### Train a tuned logistic regression model

Train a logistic regression model using the full training data set and the parameters that rolled out of the cross validation with hyper model parameter tuning.

```{r}
### Train a logistic regression model ###
glm_tuned_start_time <- Sys.time()
glm_model <- ml_logistic_regression(train_data, "delay ~ .",
                                    elastic_net_param = a[which.max(a$accuracy),"elastic_net_param_1"],
                                    reg_param = a[which.max(a$accuracy),"reg_param_1"])
glm_tuned_end_time <- Sys.time()
glm_tuned_train_time <- glm_tuned_end_time - glm_tuned_start_time
### Performance on train set
augment_value <- augment(glm_model) %>% collect()
augment_value$delay <- as.factor(augment_value$delay)
augment_value$.predicted_label <- as.factor(augment_value$.predicted_label)

glm_tuned_result_train <- augment_value %>%
  accuracy(estimate = .predicted_label, truth = delay)

glm_tuned_result_train

glm_tuned_roc_train <- augment_value %>%
  mutate(.predicted_label = as.numeric(as.character(.predicted_label))) %>%
  roc_auc(estimate = .predicted_label, truth = delay)

ml_save(glm_model, "models/glm_model", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
glm_tuned_result <- ml_predict(glm_model,test_data) %>%
  ml_metrics_multiclass()

glm_tuned_roc_test <- ml_predict(glm_model,test_data) %>%
  ml_metrics_binary()

glm_tuned_roc_test[1,]$.estimate

glm_tuned_result
```

## Decision tree model

In this section we will build a decision tree pipeline and cross-validate and hyper-parameter tune a decision tree model.

### Building a decision tree pipeline

```{r}
# Pipeline
tree_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_decision_tree_classifier()

# Grid
grid <- list(decision_tree = list(max_depth = c(1,3,5,7,10)))

# Cross validation
cv <- ml_cross_validator(
  sc,
  estimator = tree_pipeline, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_multiclass_classification_evaluator(sc,metric_name = "accuracy"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
)

tree_start_time <- Sys.time()
cv_model <- ml_fit(cv, train_data)
tree_end_time <- Sys.time()
tree_train_time <- tree_end_time - tree_start_time

a <- cv_model$avg_metrics_df
tree_cv_result <- a[which.max(a$accuracy),"accuracy"]
```


### Train a decision tree model

Train a decision tree model using the train dataset and get model performance on training data.

```{r}
### Train a decision tree model ###
tree_tuned_start_time <- Sys.time()
tree_model <- ml_decision_tree_classifier(train_data, 
                                          "delay ~ .",
                                          max_depth = a[which.max(a$accuracy),"max_depth_1"])
tree_tuned_end_time <- Sys.time()
tree_tuned_train_time <- tree_tuned_end_time - tree_tuned_start_time

### Performance on train set
augment_value <- augment(tree_model) %>% collect()
augment_value$delay <- as.factor(augment_value$delay)
augment_value$.predicted_label <- as.factor(augment_value$.predicted_label)

tree_tuned_result_train <- augment_value %>%
  accuracy(estimate = .predicted_label, truth = delay)
tree_tuned_result_train

tree_tuned_roc_train <- augment_value %>%
  mutate(.predicted_label = as.numeric(as.character(.predicted_label))) %>%
  roc_auc(estimate = .predicted_label, truth = delay)

ml_save(tree_model, "models/tree_model_classification", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
tree_tuned_result <- ml_predict(tree_model,test_data) %>%
  ml_metrics_multiclass()

tree_tuned_roc_test <- ml_predict(tree_model,test_data) %>%
  ml_metrics_binary()

tree_tuned_result
```

## Random forest model

In this section we will build a random forest pipeline and cross-validate and hyper-parameter tune a random forest model.

### Building a random forest pipeline

```{r}
# Pipeline
rf_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_random_forest_classifier()

# Grid
grid <- list(random_forest = list(max_depth = c(1,3,5,7,10), num_trees = c(1,3,5,7,10,25,50)))

# Cross validation
cv <- ml_cross_validator(
  sc,
  estimator = rf_pipeline, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_multiclass_classification_evaluator(sc,metric_name = "accuracy"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
)

rf_start_time <- Sys.time()
cv_model <- ml_fit(cv, train_data)
rf_end_time <- Sys.time()
rf_train_time <- rf_end_time - rf_start_time

a <- cv_model$avg_metrics_df
rf_cv_result <- a[which.max(a$accuracy),"accuracy"]
```

### Train a random forest model

Train a random forest model using the train dataset and get model performance on training data.

```{r}
### Train a decision tree model ###
rf_tuned_start_time <- Sys.time()
rf_model <- ml_random_forest_classifier(train_data, 
                                        "delay ~ .", 
                                        num_trees = a[which.max(a$accuracy),"num_trees_1"],
                                        max_depth = a[which.max(a$accuracy),"max_depth_1"])
rf_tuned_end_time <- Sys.time()
rf_tuned_train_time <- rf_tuned_end_time - rf_tuned_start_time

### Performance on train set
augment_value <- augment(rf_model) %>% collect()
augment_value$delay <- as.factor(augment_value$delay)
augment_value$.predicted_label <- as.factor(augment_value$.predicted_label)

rf_tuned_result_train <- augment_value %>%
  accuracy(estimate = .predicted_label, truth = delay)
rf_tuned_result_train

rf_tuned_roc_train <- augment_value %>%
  mutate(.predicted_label = as.numeric(as.character(.predicted_label))) %>%
  roc_auc(estimate = .predicted_label, truth = delay)

ml_save(rf_model, "models/rf_model_classification", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
rf_tuned_result <- ml_predict(rf_model,test_data) %>%
  ml_metrics_multiclass()

rf_tuned_roc_test <- ml_predict(rf_model,test_data) %>%
  ml_metrics_binary()

rf_tuned_result
```

# Numerical prediction of the delay time

In this section we will build and evaluate different machine learning models to predict the amount of delay an inbound flight of the United States will have.

## Random Forest Model

In this section we will build a random forest pipeline and cross-validate and hyper-parameter tune a random forest model for the numerical delay prediction.

### Building a random forest pipeline

```{r}
# Pipeline
rf_pipeline_regression <- ml_pipeline(sc) %>%
  ft_r_formula(delay_time ~ .) %>%
  ml_random_forest_regressor()

# Grid
grid <- list(random_forest = list(max_depth = c(1,3,5,7,10), num_trees = c(1,3,5,7,10,25,50)))

cv <- ml_cross_validator(
  sc,
  estimator = rf_pipeline_regression, # use our pipeline to estimate the model
  estimator_param_maps = grid, # use the params in grid
  evaluator = ml_regression_evaluator(sc,metric_name = "r2"), # how to evaluate the CV
  num_folds = 4, # number of CV folds
  seed = 2018
 )

rf_start_time_regr <- Sys.time()
cv_model <- ml_fit(cv, train_data_regr)
rf_end_time_regr <- Sys.time()
rf_train_time_regr <- rf_end_time_regr - rf_start_time_regr

a <- cv_model$avg_metrics_df

rf_cv_result_regr <- a[which.max(a$r2),"r2"]
```

### Train a random forest model

Train a random forest model using the train dataset and get model performance on training data.

```{r}
### Train a random forest model ###
rf_regr_tuned_start_time <- Sys.time()
rf_model_regr <- ml_random_forest_regressor(train_data_regr,
                                        "delay_time ~ .",
                                        num_trees = a[which.max(a$r2),"num_trees_1"],
                                        max_depth = a[which.max(a$r2),"max_depth_1"])
rf_regr_tuned_end_time <- Sys.time()
rf_regr_tuned_train_time <- rf_regr_tuned_end_time - rf_regr_tuned_start_time


### Performance on train set
augment_value <- augment(rf_model_regr) %>% collect()
metric_flights <- metric_set(rsq, mae, rmse)

rf_regr_tuned_result_train <- metric_flights(augment_value, truth = delay_time, estimate = .prediction)

rf_regr_tuned_result_train

ml_save(rf_model_regr, "models/rf_model_regression", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
rf_regr_tuned_result_test <- ml_predict(rf_model_regr,test_data_regr) %>%
  ml_metrics_regression(truth = delay_time, estimate = prediction)

rf_regr_tuned_result_test
```

# Summary of results

## Cross validation

### Classification

```{r}
model_type <- c("glm", "decision tree", "random forest")
train_time <- c(glm_train_time, tree_train_time, rf_train_time)
cv_results <- c(glm_cv_result, tree_cv_result, rf_cv_result)

classification_results <- data.frame(model_type, train_time, cv_results)
colnames(classification_results) <- c("model type", "train time", "cv accuracy")
classification_results
```

### Numerical prediction

```{r}
model_type <- c("random forest")
train_time <- c(rf_train_time_regr)
cv_results <- c(rf_cv_result_regr)

numerical_prediction_results <- data.frame(model_type, train_time, cv_results)
colnames(numerical_prediction_results) <- c("model type", "train time", "cv r-squared")
numerical_prediction_results
```

## Final tuned models

### Classification

```{r}
model_type <- c("glm", "decision tree", "random forest")
train_time <- c(glm_tuned_train_time, tree_tuned_train_time, rf_tuned_train_time)
tuned_results_train <- c(glm_tuned_result_train$.estimate, tree_tuned_result_train$.estimate, rf_tuned_result_train$.estimate)
tuned_results_test <- c(glm_tuned_result$.estimate, tree_tuned_result$.estimate, rf_tuned_result$.estimate)
tuned_roc_train <- c(glm_tuned_roc_train[1,]$.estimate, tree_tuned_roc_train[1,]$.estimate, rf_tuned_roc_train[1,]$.estimate)
tuned_roc_test <- c(glm_tuned_roc_test[1,]$.estimate, tree_tuned_roc_test[1,]$.estimate, rf_tuned_roc_test[1,]$.estimate)

classification_results <- data.frame(model_type, train_time, tuned_results_train, tuned_results_test, tuned_roc_train, tuned_roc_test)
colnames(classification_results) <- c("model type", "train time", "Acc. train set", "Acc. test set", "ROC train set", "ROC test set")
classification_results
```

### Numerical prediction

```{r}
model_type <- c("random forest")
train_time <- c(rf_regr_tuned_train_time)
tuned_results_train <- c(rf_regr_tuned_result_train[1,]$.estimate)
tuned_results_test <- c(rf_regr_tuned_result_test[2,]$.estimate) ###

numerical_prediction_results <- data.frame(model_type, train_time, tuned_results_train, tuned_results_test)
colnames(numerical_prediction_results) <- c("model type", "train time", "R2 train set", "R2 test set")
numerical_prediction_results
```

