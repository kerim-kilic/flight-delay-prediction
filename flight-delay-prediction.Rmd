---
title: "Study of delay prediction in the US airport network"
author: "Kerim Kili√ß"
subtitle: "Supervised Machined Learning using flight data"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 2
    number_sections: true
    toc_float: true
---

# Libraries

The following libraries are used in this R markdown file.

```{r setup, message=FALSE}
markdown_start_time <- Sys.time()
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidymodels)
library(sparklyr)
library(kableExtra)
source("src/functions.R")
```

# Initialize spark and read in data

```{r}
### Check if spark installation exists, if not install correct version
if(!spark_install_find("3.3.0")$installed)
{
  spark_install(version = "3.3.0")
}

spark_config <- spark_config()
spark_config$'sparklyr.shell.driver-memory' <- "12G"

sc <- spark_connect(master = "local",
                    config = spark_config,
                    version = "3.3.0")

### Check if csv of the data file exists, if not create one.
if(!file.exists("data/flights_2017.csv"))
{
  my_data <- readRDS("data/flights_2017.RDS")
  fwrite(my_data,file = "data/flights_2017.csv")  
}

raw_data <- spark_read_csv(sc,"data/flights_2017.csv")
raw_data %>% glimpse()
```

# Create datacleaning pipeline

```{r}
# Create the main pipeline
# Remove unnecessary variables
# Remove any row with any NA value
# Classify flights with a delay with "1",  and no delay with "0"
main_pipeline <- .%>%
  na.omit() %>%
  mutate(delay_time = actual_arrival_time - planned_arrival_time,
         delay_time = minute(delay_time) + (hour(delay_time)*60) + (second(delay_time)/60),
         delay = case_when(delay_time > 0 ~ "1",
                           delay_time <= 0 ~ "0")) %>%
  select(quarter,month,day_of_month,day_of_week,hour_of_day,minutes_of_hour,
         planned_departure_local_hour,planned_arrival_local_hour,
         # wheels_off_time_local_hour,wheels_on_time_local_hour,
         flight_distance,seating_capacity,
         delay,delay_time)

# Create the numerical delay sub-pipeline
# Sum the individual types of delay to a total delay_time.
# Remove individual delays
numer_delay_pipeline <- .%>% main_pipeline %>% 
  select(-delay)

# Create the classification sub-pipeline
# Remove the individual delays 
class_delay_pipeline <- .%>% main_pipeline %>% 
  select(-delay_time) %>%
  group_by(delay)
```

# Splitting data in train and test sets

Take a sample set of the data to reduce time to train the model.

```{r, message=FALSE}
# Split for classification
classification_split <- create_train_test_split(data = class_delay_pipeline(raw_data),
                                                sample_size = 100000,
                                                ratio = 0.8,
                                                type = "classification")
train_data <- classification_split$train_data
test_data <- classification_split$test_data
# Split for numerical predictions
numerical_split <- create_train_test_split(data = numer_delay_pipeline(raw_data),
                                      sample_size = 100000,
                                      ratio = 0.8,
                                      type = "numerical")
train_data_regr <- numerical_split$train_data
test_data_regr <- numerical_split$test_data

rm(raw_data)
```

Let's glimpse into the train data for classification.

```{r}
train_data %>% glimpse()
```

Let's glimpse into the train data for numerical prediction.

```{r}
train_data_regr %>% glimpse()
```


# Classification of flight delays

In this section we will build and evaluate different machine learning models to predict if a given inbound flight in the United States will have a delay based on the data prepared in the previous sections.

## Logistic regression model

In this section we will build a logistic regression pipeline and cross-validate and hyper-parameter tune a logistic regression model.

### Building a logistic regression pipeline 

Below we build a ML pipeline for a logistic regression model to use cross validation as we perform hyper parameter tuning on our model.

```{r}
# Pipeline
glm_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_logistic_regression()

# Grid
grid <- list(logistic_regression = list(elastic_net_param = c(0,0.25,0.5,0.75,1), reg_param = c(0,0.25,0.5,0.75,1)))

# Cross validate model
glm_cv <- cross_validator(sc = sc,
                          data = train_data,
                          pipeline = glm_pipeline,
                          grid = grid,
                          type = "classification",
                          folds = 4,
                          seed = 2018)

# Get model results
a <- glm_cv$all_results
glm_cv_best_result <- glm_cv$best_result
glm_cv_result <- glm_cv_best_result[which.max(glm_cv_best_result$accuracy),"accuracy"]
glm_train_time <- glm_cv$train_time
a[order(-a$accuracy),] %>% 
  head(5) %>%
  kbl() %>%
  kable_minimal()
```


### Train a tuned logistic regression model

Train a logistic regression model using the full training data set and the parameters that rolled out of the cross validation with hyper model parameter tuning.

```{r}
### Train a logistic regression model ###
glm_tuned_start_time <- Sys.time()
glm_model <- ml_logistic_regression(train_data, "delay ~ .",
                                    elastic_net_param = glm_cv_best_result[which.max(glm_cv_best_result$accuracy),"elastic_net_param_1"],
                                    reg_param = glm_cv_best_result[which.max(glm_cv_best_result$accuracy),"reg_param_1"])
glm_tuned_end_time <- Sys.time()
glm_tuned_train_time <- glm_tuned_end_time - glm_tuned_start_time
### Performance on train set

glm_tuned_result_train <- generate_metrics_classification(model = glm_model,
                                                          type = "train")

glm_tuned_result_train %>% 
  kbl() %>%
  kable_minimal()
```

Save the model to be able to reuse it later on predictions.

```{r, message=FALSE}
ml_save(glm_model, "models/glm_model", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
glm_tuned_result <- generate_metrics_classification(glm_model,
                                                    "test",
                                                    test_data)

glm_tuned_result %>% 
  kbl() %>%
  kable_minimal()
```

## Decision tree model

In this section we will build a decision tree pipeline and cross-validate and hyper-parameter tune a decision tree model.

### Building a decision tree pipeline

```{r}
# Pipeline
tree_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_decision_tree_classifier()

# Grid
grid <- list(decision_tree = list(max_depth = c(1,3,5,7,10)))

# Cross validate model
tree_cv <- cross_validator(sc = sc,
                           data = train_data,
                           pipeline = tree_pipeline,
                           grid = grid,
                           type = "classification",
                           folds = 4,
                           seed = 2018)

# Get model results
a <- tree_cv$all_results
tree_cv_best_result <- tree_cv$best_result
tree_cv_result <- tree_cv_best_result[which.max(tree_cv_best_result$accuracy),"accuracy"]
tree_train_time <- tree_cv$train_time
a[order(-a$accuracy),] %>% 
  head(5) %>%
  kbl() %>%
  kable_minimal()
```


### Train a decision tree model

Train a decision tree model using the train dataset and get model performance on training data.

```{r}
### Train a decision tree model ###
tree_tuned_start_time <- Sys.time()
tree_model <- ml_decision_tree_classifier(train_data, 
                                          "delay ~ .",
                                          max_depth = tree_cv_best_result[which.max(tree_cv_best_result$accuracy),"max_depth_1"])
tree_tuned_end_time <- Sys.time()
tree_tuned_train_time <- tree_tuned_end_time - tree_tuned_start_time

### Performance on train set
tree_tuned_result_train <- generate_metrics_classification(tree_model,"train")

tree_tuned_result_train %>% 
  kbl() %>%
  kable_minimal()
```

Save the model to be able to reuse it later on predictions.

```{r, message=FALSE}
ml_save(tree_model, "models/tree_model_classification", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
tree_tuned_result <- generate_metrics_classification(tree_model,
                                                    "test",
                                                    test_data)

tree_tuned_result %>% 
  kbl() %>%
  kable_minimal()
```

## Random forest model

In this section we will build a random forest pipeline and cross-validate and hyper-parameter tune a random forest model.

### Building a random forest pipeline

```{r}
# Pipeline
rf_pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(delay ~ .) %>%
  ml_random_forest_classifier()

# Grid
grid <- list(random_forest = list(max_depth = c(1,3,5,7,10), num_trees = c(1,3,5,7,10,25,50)))

# Cross validate model
rf_cv <- cross_validator(sc = sc,
                         data = train_data,
                         pipeline = rf_pipeline,
                         grid = grid,
                         type = "classification",
                         folds = 4,
                         seed = 2018)

# Get model results
a <- rf_cv$all_results
rf_cv_best_result <- rf_cv$best_result
rf_cv_result <- rf_cv_best_result[which.max(rf_cv_best_result$accuracy),"accuracy"]
rf_train_time <- rf_cv$train_time
a[order(-a$accuracy),] %>% 
  head(5) %>%
  kbl() %>%
  kable_minimal()
```

### Train a random forest model

Train a random forest model using the train dataset and get model performance on training data.

```{r}
### Train a decision tree model ###
rf_tuned_start_time <- Sys.time()
rf_model <- ml_random_forest_classifier(train_data, 
                                        "delay ~ .", 
                                        num_trees = rf_cv_best_result[which.max(rf_cv_best_result$accuracy),"num_trees_1"],
                                        max_depth = rf_cv_best_result[which.max(rf_cv_best_result$accuracy),"max_depth_1"])
rf_tuned_end_time <- Sys.time()
rf_tuned_train_time <- rf_tuned_end_time - rf_tuned_start_time

### Performance on train set
rf_tuned_result_train <- generate_metrics_classification(rf_model,"train")

rf_tuned_result_train %>% 
  kbl() %>%
  kable_minimal()
```

Save the model to be able to reuse it later on predictions.

```{r, message=FALSE}
ml_save(rf_model, "models/rf_model_classification", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
rf_tuned_result <- generate_metrics_classification(rf_model,
                                                    "test",
                                                    test_data)

rf_tuned_result %>% 
  kbl() %>%
  kable_minimal()
```

# Numerical prediction of the delay time

In this section we will build and evaluate different machine learning models to predict the amount of delay an inbound flight of the United States will have.

## Random Forest Model

In this section we will build a random forest pipeline and cross-validate and hyper-parameter tune a random forest model for the numerical delay prediction.

### Building a random forest pipeline

```{r}
# Pipeline
rf_pipeline_regression <- ml_pipeline(sc) %>%
    ft_r_formula(delay_time ~ .) %>%
    ml_random_forest_regressor()

# Grid
grid <- list(random_forest = list(max_depth = c(1,3,5,7,10), num_trees = c(1,3,5,7,10,25,50)))

# Cross validate model
rf_cv <- cross_validator(sc = sc,
                         data = train_data_regr,
                         pipeline = rf_pipeline_regression,
                         grid = grid,
                         type = "numerical",
                         folds = 4,
                         seed = 2018)

# Get model results
a <- rf_cv$all_results
rf_cv_best_result_regr <- rf_cv$best_result 
rf_cv_result_regr <- rf_cv_best_result_regr[which.max(rf_cv_best_result_regr$r2),"r2"]
rf_train_time_regr <- rf_cv$train_time
a[order(-a$r2),] %>% 
  head(5) %>%
  kbl() %>%
  kable_minimal()
```

### Train a random forest model

Train a random forest model using the train dataset and get model performance on training data.

```{r}
### Train a random forest model ###
rf_regr_tuned_start_time <- Sys.time()
rf_model_regr <- ml_random_forest_regressor(train_data_regr,
                                        "delay_time ~ .",
                                        num_trees = rf_cv_best_result_regr[which.max(rf_cv_best_result_regr$r2),"num_trees_1"],
                                        max_depth = rf_cv_best_result_regr[which.max(rf_cv_best_result_regr$r2),"max_depth_1"])
rf_regr_tuned_end_time <- Sys.time()
rf_regr_tuned_train_time <- rf_regr_tuned_end_time - rf_regr_tuned_start_time

### Performance on train set
augment_value <- augment(rf_model_regr) %>% collect()
metric_flights <- metric_set(rsq, mae, rmse)

rf_regr_tuned_result_train <- metric_flights(augment_value, truth = delay_time, estimate = .prediction)

rf_regr_tuned_result_train %>% 
  kbl() %>%
  kable_minimal()
```

Save the model to be able to reuse it later on predictions.

```{r, message=FALSE}
ml_save(rf_model_regr, "models/rf_model_regression", overwrite = TRUE)
```

Make a prediction based on test data and get model performance.

```{r}
### Performance on test set
rf_regr_tuned_result_test <- ml_predict(rf_model_regr,test_data_regr) %>%
  ml_metrics_regression(truth = delay_time, estimate = prediction)

rf_regr_tuned_result_test %>% 
  kbl() %>%
  kable_minimal()
```

# Summary of results

## Cross validation

### Classification

```{r}
model_type <- c("glm", "decision tree", "random forest")
train_time <- c(glm_train_time, tree_train_time, rf_train_time)
cv_results <- c(glm_cv_result, tree_cv_result, rf_cv_result)

classification_results <- data.frame(model_type, train_time, cv_results)
colnames(classification_results) <- c("model type", "train time", "cv accuracy")
classification_results %>% 
  kbl() %>%
  kable_minimal()
```

### Numerical prediction

```{r}
model_type <- c("random forest")
train_time <- c(rf_train_time_regr)
cv_results <- c(rf_cv_result_regr)

numerical_prediction_results <- data.frame(model_type, train_time, cv_results)
colnames(numerical_prediction_results) <- c("model type", "train time", "cv r-squared")
numerical_prediction_results %>% 
  kbl() %>%
  kable_minimal()
```

## Final tuned models

### Classification

Summary of model metrics on the train set

```{r, echo=FALSE}
model_type <- c("glm", "decision tree", "random forest")
train_time <- round(c(glm_tuned_train_time, tree_tuned_train_time, rf_tuned_train_time),3)

accuracy_results_train <- c(glm_tuned_result_train[1,]$value, tree_tuned_result_train[1,]$value, rf_tuned_result_train[1,]$value)
recall_results_train <- c(glm_tuned_result_train[2,]$value, tree_tuned_result_train[2,]$value, rf_tuned_result_train[2,]$value)
precision_results_train <- c(glm_tuned_result_train[3,]$value, tree_tuned_result_train[3,]$value, rf_tuned_result_train[3,]$value)
missclassification_results_train<- c(glm_tuned_result_train[4,]$value, tree_tuned_result_train[4,]$value, rf_tuned_result_train[4,]$value)
specificity_results_train <- c(glm_tuned_result_train[5,]$value, tree_tuned_result_train[5,]$value, rf_tuned_result_train[5,]$value)
f1_results_train <- c(glm_tuned_result_train[6,]$value, tree_tuned_result_train[6,]$value, rf_tuned_result_train[6,]$value)
roc_auc_results_train <- c(glm_tuned_result_train[7,]$value, tree_tuned_result_train[7,]$value, rf_tuned_result_train[7,]$value)
pr_auc_results_train <- c(glm_tuned_result_train[8,]$value, tree_tuned_result_train[8,]$value, rf_tuned_result_train[8,]$value)

classification_results_train <- data.frame(model_type, 
                                     train_time, 
                                     accuracy_results_train, 
                                     recall_results_train,
                                     precision_results_train,
                                     missclassification_results_train,
                                     specificity_results_train,
                                     f1_results_train,
                                     roc_auc_results_train,
                                     pr_auc_results_train)

colnames(classification_results_train) <- c("model type", 
                                      "train time", 
                                      "accuracy", 
                                      "recall",
                                      "precision",
                                      "missclassification",
                                      "specificity",
                                      "f1",
                                      "roc_auc",
                                      "pr_auc")
classification_results_train %>% 
  kbl() %>%
  kable_minimal()
```

Summary of model metrics on the test set

```{r, echo=FALSE}
accuracy_results_test <- c(glm_tuned_result[1,]$value, tree_tuned_result[1,]$value, rf_tuned_result[1,]$value)
recall_results_test <- c(glm_tuned_result[2,]$value, tree_tuned_result[2,]$value, rf_tuned_result[2,]$value)
precision_results_test <- c(glm_tuned_result[3,]$value, tree_tuned_result[3,]$value, rf_tuned_result[3,]$value)
missclassification_results_test <- c(glm_tuned_result[4,]$value, tree_tuned_result[4,]$value, rf_tuned_result[4,]$value)
specificity_results_test <- c(glm_tuned_result[5,]$value, tree_tuned_result[5,]$value, rf_tuned_result[5,]$value)
f1_results_test <- c(glm_tuned_result[6,]$value, tree_tuned_result[6,]$value, rf_tuned_result[6,]$value)
roc_auc_results_test <- c(glm_tuned_result[7,]$value, tree_tuned_result[7,]$value, rf_tuned_result[7,]$value)
pr_auc_results_test <- c(glm_tuned_result[8,]$value, tree_tuned_result[8,]$value, rf_tuned_result[8,]$value)

classification_results_train <- data.frame(model_type, 
                                     accuracy_results_test,
                                     recall_results_test,
                                     precision_results_test,
                                     missclassification_results_test,
                                     specificity_results_test,
                                     f1_results_test,
                                     roc_auc_results_test,
                                     pr_auc_results_test)
colnames(classification_results_train) <- c("model",
                                      "accuracy", 
                                      "recall",
                                      "precision",
                                      "missclassification",
                                      "specificity",
                                      "f1",
                                      "roc_auc",
                                      "pr_auc")
classification_results_train %>% 
  kbl() %>%
  kable_minimal()
```

### Numerical prediction

Summary of model metrics on the train set

```{r, echo=FALSE}
model_type <- c("random forest")
train_time <- c(rf_regr_tuned_train_time)
r2_results_train <- c(rf_regr_tuned_result_train[1,]$.estimate)
mae_results_train <- c(rf_regr_tuned_result_train[2,]$.estimate)
rmse_results_train <- c(rf_regr_tuned_result_train[3,]$.estimate)

numerical_prediction_results <- data.frame(model_type, train_time, r2_results_train, mae_results_train, rmse_results_train)
colnames(numerical_prediction_results) <- c("model type", "train time", "R2", "MAE", "RMSE")

numerical_prediction_results %>% 
  kbl() %>%
  kable_minimal()
```

Summary of model metrics on the test set

```{r, echo=FALSE}
r2_results_test <- c(rf_regr_tuned_result_test[2,]$.estimate)
mae_results_test <- c(rf_regr_tuned_result_test[3,]$.estimate)
rmse_results_test <- c(rf_regr_tuned_result_test[1,]$.estimate)

numerical_prediction_results <- data.frame(model_type, train_time, r2_results_test, mae_results_test, rmse_results_test)
colnames(numerical_prediction_results) <- c("model type", "train time", "R2", "MAE", "RMSE")

numerical_prediction_results %>% 
  kbl() %>%
  kable_minimal()
markdown_end_time <- Sys.time()
```

The total time to knit this markdown is:

```{r}
markdown_end_time - markdown_start_time
```